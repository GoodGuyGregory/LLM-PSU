{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051412af-c8e9-46be-90ea-9966e47db70d",
   "metadata": {},
   "source": [
    "# Assignment 1: Large Language Models for Text Generation\n",
    "### CS 410/510 Large Language Models Fall 2024\n",
    "#### Greg Witt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c8544-965b-41c7-9c42-1232265ea4bf",
   "metadata": {},
   "source": [
    "**Q1. Describe three differences between Llama 3.2 models and Phi-3.5 model.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c29179-cec8-417b-9f83-c2e895bdabc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410cb3c6-93db-4d8a-a680-69118ed2bee5",
   "metadata": {},
   "source": [
    "**Q2. Generate a story of 200 words that starts with the words *“Once upon a time”* using each of these models.**\n",
    "**You should have 3 outputs in total.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9aabfb-4720-4baf-b2ad-cb32f59b100e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1549904016.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install transformers\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# required packages\n",
    "# pip install transformers\n",
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9787f260-5c1c-472a-99e2-7b827ce6072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable the models can use.\n",
    "story_prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f3c4f-00d1-4b6e-92a5-6006da1b7612",
   "metadata": {},
   "source": [
    "**Llama 3.2 - 1B:**\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b235451-866b-42c0-99d0-64446bf8d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time 200 years ago, the Russian Army was on its way to invade Japan. However in order to do so it needed Japanese territory for troops and ships along with supplies of foodstuffs as well.\n",
      "The Russians had already started laying out military bases (along both coasts) that could be used by their army before they were even invaded by Russia proper at Port Arthur which took place just two days after Japan's declaration against Russian invasion on July 26th - August I don't remember exactly how long it took them then but it wasn't much longer than three weeks later when war broke out between Japan/Russia/Manchuria/Tibetan Empire.\n",
      "I think some sort of treaty might have been negotiated because we know there was one made soon afterward during this period where the Russians agreed not to annex Manchu land or anything like that while still maintaining control over parts of Korea etc...\n",
      "And now, fast forward again around ten years or more from right about 1932\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_1B = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_1B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_1B, torch_dtype=torch.float16)\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "# Your special prompt\n",
    "\n",
    "\n",
    "story_prompt = \"Once upon a time \"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-1B\n",
    "outputs = model.generate(prompt_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         max_length=200,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         pad_token_id=tokenizer.eos_token_id,\n",
    "                         temperature=0.93,\n",
    "                         top_k=30,\n",
    "                         top_p=0.90,\n",
    "                         repetition_penalty=1.2\n",
    "                        )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0]\n",
    "\n",
    "generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c01c09-fb8e-4274-bba2-11192240b20a",
   "metadata": {},
   "source": [
    "**Llama 3.2 - 3B:**\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85770997-9892-4d17-97e2-9c04c7f5fbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37daaa670827402dae2615baf8b4a7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ea8947d1d94ba3a867dd6cc9e61a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ending with 'The end'.\n",
      "Once upon a time there was a farmer who had three cows, and he wanted to sell them. But he couldn't, because they were all sick and had to be put down.\n",
      "The farmer's wife went to the doctor, but he couldn't help them. She tried the vet, but they were too expensive. She even went to the local hospital, but it was full. So she decided to go to the emergency room at the nearest big city hospital. There, a doctor told her that the cows were too far gone to be saved.\n",
      "The farmer's wife was so disappointed that she went home and cried. But she had to think of a way to make money. She decided to sell some of her furniture. She sold the dining table, the sofa, the TV, and the stereo. But they didn't sell very well. She even tried to sell the car, but no one wanted it.\n",
      "The farmer's wife was sad, but she knew she had to make a living somehow. So she went to the bank and got a loan. But the bank didn't give her enough money, so she had to get a second loan. But the second bank wouldn't give her enough either, so she had to get a third loan. But the third bank wouldn't give her enough, so she had to get a fourth loan. But the fourth bank wouldn't\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_3B = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_3B)\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_3B, torch_dtype=torch.float32)\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "# Your special prompt\n",
    "\n",
    "\n",
    "story_prompt = \"Tell me a story of 200 words starting with 'Once upon a time'\"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-3B\n",
    "outputs = model.generate(prompt_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=300,\n",
    "                        do_sample=True,\n",
    "                        num_return_sequences=1,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        temperature=0.85,\n",
    "                        top_k=35,\n",
    "                        repetition_penalty=1\n",
    "                    )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0][len(prompt_ids[0]):]\n",
    "\n",
    "generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70271cf-668e-4065-a838-783846cac664",
   "metadata": {},
   "source": [
    "**Phi 3.5-Mini-Instruct:**\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708f995-eaae-4700-9275-eb1aec36d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "phi_35_mini_inst = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(phi_35_mini_inst)\n",
    "model = AutoModelForCausalLM.from_pretrained(phi_35_mini_inst)\n",
    "\n",
    "\n",
    "input_text = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(inputs.input_ids, max_length=300, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
