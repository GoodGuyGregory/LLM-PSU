{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051412af-c8e9-46be-90ea-9966e47db70d",
   "metadata": {},
   "source": [
    "# Assignment 1: Large Language Models for Text Generation\n",
    "### CS 410/510 Large Language Models Fall 2024\n",
    "#### Greg Witt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c8544-965b-41c7-9c42-1232265ea4bf",
   "metadata": {},
   "source": [
    "**Q1. Describe three differences between Llama 3.2 models and Phi-3.5 model.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c29179-cec8-417b-9f83-c2e895bdabc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410cb3c6-93db-4d8a-a680-69118ed2bee5",
   "metadata": {},
   "source": [
    "**Q2. Generate a story of 200 words that starts with the words *“Once upon a time”* using each of these models.**\n",
    "**You should have 3 outputs in total.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9aabfb-4720-4baf-b2ad-cb32f59b100e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1549904016.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install transformers\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# required packages\n",
    "# pip install transformers\n",
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9787f260-5c1c-472a-99e2-7b827ce6072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable the models can use.\n",
    "story_prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f3c4f-00d1-4b6e-92a5-6006da1b7612",
   "metadata": {},
   "source": [
    "**Llama 3.2 - 1B:**\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b235451-866b-42c0-99d0-64446bf8d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time 200 years ago, the Russian Army was on its way to invade Japan. However in order to do so it needed Japanese territory for troops and ships along with supplies of foodstuffs as well.\n",
      "The Russians had already started laying out military bases (along both coasts) that could be used by their army before they were even invaded by Russia proper at Port Arthur which took place just two days after Japan's declaration against Russian invasion on July 26th - August I don't remember exactly how long it took them then but it wasn't much longer than three weeks later when war broke out between Japan/Russia/Manchuria/Tibetan Empire.\n",
      "I think some sort of treaty might have been negotiated because we know there was one made soon afterward during this period where the Russians agreed not to annex Manchu land or anything like that while still maintaining control over parts of Korea etc...\n",
      "And now, fast forward again around ten years or more from right about 1932\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_1B = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_1B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_1B, torch_dtype=torch.float16)\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "# Your special prompt\n",
    "\n",
    "\n",
    "story_prompt = \"Once upon a time \"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-1B\n",
    "outputs = model.generate(prompt_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         max_length=200,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         pad_token_id=tokenizer.eos_token_id,\n",
    "                         temperature=0.93,\n",
    "                         top_k=30,\n",
    "                         top_p=0.90,\n",
    "                         repetition_penalty=1.2\n",
    "                        )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0]\n",
    "\n",
    "generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c01c09-fb8e-4274-bba2-11192240b20a",
   "metadata": {},
   "source": [
    "**Llama 3.2 - 3B:**\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85770997-9892-4d17-97e2-9c04c7f5fbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eddc2a1579e41bdac9fdae214693788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e52cb87bc7459884b57e4daa045f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89add625c423455586cb10cfbb23b08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49376cc419c045c496be7e98bb6955f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da11a04876e24585aa8d9c1994474b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "llama_32_3B = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_3B)\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_3B, torch_dtype=torch.float32)\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "# Your special prompt\n",
    "\n",
    "\n",
    "story_prompt = \"Tell me a story of 200 words starting with 'Once upon a time'\"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-3B\n",
    "outputs = model.generate(prompt_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         max_length=300,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         pad_token_id=tokenizer.eos_token_id,\n",
    "                         temperature=0.85,\n",
    "                         top_k=35,\n",
    "                         repetition_penalty=1\n",
    "                        )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0][len(prompt_ids[0]):]\n",
    "\n",
    "generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70271cf-668e-4065-a838-783846cac664",
   "metadata": {},
   "source": [
    "**Phi 3.5-Mini-Instruct:**\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708f995-eaae-4700-9275-eb1aec36d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_35_mini_inst = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
