{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051412af-c8e9-46be-90ea-9966e47db70d",
   "metadata": {},
   "source": [
    "# Assignment 1: Large Language Models for Text Generation\n",
    "### CS 410/510 Large Language Models Fall 2024\n",
    "#### Greg Witt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c8544-965b-41c7-9c42-1232265ea4bf",
   "metadata": {},
   "source": [
    "### Q1. Describe three differences between Llama 3.2 models and Phi-3.5 model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c29179-cec8-417b-9f83-c2e895bdabc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410cb3c6-93db-4d8a-a680-69118ed2bee5",
   "metadata": {},
   "source": [
    "### Q2. Generate a story of 200 words that starts with the words *“Once upon a time”* using each of these models.  \n",
    "**You should have 3 outputs in total.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8b1db",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aabfb-4720-4baf-b2ad-cb32f59b100e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# pip install transformers\n",
    "# pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f3c4f-00d1-4b6e-92a5-6006da1b7612",
   "metadata": {},
   "source": [
    "### Llama 3.2 - 1B:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a9a35-4f05-4f07-a00f-e028db3eb618",
   "metadata": {},
   "source": [
    "**Download Llama-3.2 1B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b235451-866b-42c0-99d0-64446bf8d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_1B = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_1B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_1B, torch_dtype=torch.float16)\n",
    "\n",
    "model = model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fc306-1eae-4907-8370-4c61e6064449",
   "metadata": {},
   "source": [
    "**Generate A Story with Llama 3.2 1B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c01e52-989a-4439-82dd-67a3d3ab4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was ruled by an old man who lived on Mount Olympus. One day he discovered that there were other people in his land too! He invited them all to come and live at Mount Olympus with him.\n",
      "He named these new guests: Hercules, Minos, Heracles, Atlas, Tyndareus... These are the ones we know today as the Olympian gods of Greece. They didn't like it much though!\n",
      "They weren't happy living together under one roof. So they decided to split up again. Zeus went off alone into the woods to find himself some peace from this strange life in the middle of everybody else's houses!\n",
      "Hercules stayed behind for awhile when he felt lonely. But then, after thinking about how peaceful being here would be compared to having so many people around always arguing amongst themselves...\n",
      "Then finally came along Odysseus, son of Laertes, King Eurystheus' nephew\n",
      "Odyssey - Wikipedia https://\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Our Story Prompt\n",
    "story_prompt = \"Once upon a time\"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-1B\n",
    "outputs = model.generate(prompt_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         max_length=200,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         pad_token_id=tokenizer.eos_token_id,\n",
    "                         temperature=0.93,\n",
    "                         top_k=30,\n",
    "                         top_p=0.90,\n",
    "                         repetition_penalty=1.2\n",
    "                        )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0]\n",
    "\n",
    "generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fd9a8-470d-4760-9fcb-675a985c6098",
   "metadata": {},
   "source": [
    "**Measure of Perplexity**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b29aaa-69c0-4ef3-ab17-6f0e91e6f512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Llama 3.2 1B Model: 33.89\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(prompt_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = prompt_ids[:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Llama 3.2 1B Model: {round(perplexity.item(),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c0115-5a20-460c-9aa0-209da6b7c6a6",
   "metadata": {},
   "source": [
    "**Measure Token Type Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60996167-f478-44b6-98ce-761815e4af56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Llama 3.2 1B Model: 1.0\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Once upon a time\"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Llama 3.2 1B Model:\", round(ttr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c01c09-fb8e-4274-bba2-11192240b20a",
   "metadata": {},
   "source": [
    "### Llama 3.2 - 3B:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff71f1-bd69-456d-801b-87d6aabd0fa4",
   "metadata": {},
   "source": [
    "**Download Llama 3.2 - 3B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27ce93bf-9ff2-4170-b8db-0fd3006476cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:26<00:00, 13.01s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_3B = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_3B)\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_3B, torch_dtype=torch.float32)\n",
    "\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb750382-256e-469b-854c-9c43f0fbcd26",
   "metadata": {},
   "source": [
    "**Generate A Story with Llama 3.2 3B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85770997-9892-4d17-97e2-9c04c7f5fbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time 10 years ago, I bought some beautiful hand woven rugs from an antique store. They were so soft and warm to the touch. My kids loved them! But after living in our home for several months my husband noticed that they had started getting pretty dirty.\n",
      "We tried washing the carpets with water but soon realized it was not going to work out as we wanted since there are too many of those knots on each rug.\n",
      "I decided to make carpet cleaners myself because no one has ever done this before. So here is how you can do it!\n",
      "Mix together all your ingredients except for the hot water (use cold). You may want to use about half or less than half depending on how much dirt/dust you have on these rugs.\n",
      "Pour into spray bottle. Spray liberally onto stained areas. Make sure to cover everything well.\n",
      "Wet cloth – This will help clean off excess stain & oil\n",
      "Rag – Use large rag which covers entire area\n",
      "Sponge/Soft\n"
     ]
    }
   ],
   "source": [
    "# Your special prompt\n",
    "story_prompt = \"Once upon a time \"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-3B\n",
    "outputs = model.generate(prompt_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=200,\n",
    "                        do_sample=True,\n",
    "                        num_return_sequences=1,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        temperature=0.83,\n",
    "                        top_k=30,\n",
    "                        top_p=0.90,\n",
    "                        repetition_penalty=1.2\n",
    "                    )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0]\n",
    "\n",
    "generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf467d5a-2c07-46ae-b660-3d579d7eb214",
   "metadata": {},
   "source": [
    "**Measure Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f8c739b-4676-4d26-ae16-cfbe8e5eb1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Llama 3.2 3B Model: 33.89\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(prompt_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = prompt_ids[:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Llama 3.2 3B Model: {round(perplexity.item(),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c18db-a6d4-465a-867f-e62ae3e1585b",
   "metadata": {},
   "source": [
    "**Measure Type-Token Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e97e1d68-aa02-457b-8cef-7e738360f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Llama 3.2 3B Model: 1.0\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Once upon a time\"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Llama 3.2 3B Model:\", round(ttr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70271cf-668e-4065-a838-783846cac664",
   "metadata": {},
   "source": [
    "### Phi 3.5-Mini-Instruct:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d765aff-1082-44fc-ab36-ba5a2e8e2404",
   "metadata": {},
   "source": [
    "**Download Phi 3.5 - Mini - Instruct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6708f995-eaae-4700-9275-eb1aec36d677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37489b290444dd1ad64886caf961ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "phi_35_mini_inst = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(phi_35_mini_inst)\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(phi_35_mini_inst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a359bf-0d4e-41cd-8d7f-4fb7400a1e1b",
   "metadata": {},
   "source": [
    "**Generate a Story with Phi 3.5 - Mini Instruct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28b4c3dd-93ea-4371-8143-a0c39ec85f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a 200 word story that begins with, 'Once upon a time' incorperate a nature theme and make it scary, or mysterious.\n",
      "\n",
      "Once upon a time, in a dense, mist-shrouded forest, where the trees whispered secrets and the shadows danced with an eerie grace, there existed a haunting mystery. The forest, with its ancient roots and gnarled branches, was said to be home to a spectral entity. Legends spoke of a ghostly figure, cloaked in the remnants of autumn leaves, wandering the twisted paths under the moon's pale gaze.\n",
      "\n",
      "The villagers living on the forest's edge lived in constant fear, for they knew that the entity's presence brought misfortune and despair. Children were forbidden to play near the woods, and the elders would recount tales of the ghost's sorrowful\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Generate a 200 word story that begins with, 'Once upon a time' incorperate a nature theme and make it scary, or mysterious\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = phi_tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = phi_model.generate(inputs.input_ids, \n",
    "                             max_length=200,\n",
    "                             temperature=0.85,\n",
    "                             )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_story = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90437347-5282-4ded-b722-b19cd0ba677d",
   "metadata": {},
   "source": [
    "**Measure of Perplexity**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4cd07637-1ee4-4e80-8a30-40fbcdf062d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Phi-3 Model: 24.16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = phi_model(inputs.input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs.input_ids[:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Phi-3 Model: {round(perplexity.item(),2)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16befdc5-5305-494b-b6ea-c88fd27e0b48",
   "metadata": {},
   "source": [
    "**Measure Type Token Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36e0c5d2-b221-42c4-8e60-df0e354edc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Phi-3 Model: 0.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_text = \"Generate a 200 word story that begins with, 'Once upon a time'\"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Phi-3 Model:\", round(ttr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc207a",
   "metadata": {},
   "source": [
    "## Q3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
