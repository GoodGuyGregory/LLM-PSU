{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051412af-c8e9-46be-90ea-9966e47db70d",
   "metadata": {},
   "source": [
    "# Assignment 1: Large Language Models for Text Generation\n",
    "### CS 410/510 Large Language Models Fall 2024\n",
    "#### Greg Witt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c8544-965b-41c7-9c42-1232265ea4bf",
   "metadata": {},
   "source": [
    "**Q1. Describe three differences between Llama 3.2 models and Phi-3.5 model.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c29179-cec8-417b-9f83-c2e895bdabc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410cb3c6-93db-4d8a-a680-69118ed2bee5",
   "metadata": {},
   "source": [
    "**Q2. Generate a story of 200 words that starts with the words *“Once upon a time”* using each of these models.**\n",
    "**You should have 3 outputs in total.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aabfb-4720-4baf-b2ad-cb32f59b100e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# required packages\n",
    "# pip install transformers\n",
    "# pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f3c4f-00d1-4b6e-92a5-6006da1b7612",
   "metadata": {},
   "source": [
    "### Llama 3.2 - 1B:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a9a35-4f05-4f07-a00f-e028db3eb618",
   "metadata": {},
   "source": [
    "**Download Llama-3.2 1B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b235451-866b-42c0-99d0-64446bf8d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_1B = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_1B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_1B, torch_dtype=torch.float16)\n",
    "\n",
    "model = model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fc306-1eae-4907-8370-4c61e6064449",
   "metadata": {},
   "source": [
    "**Generate A Story with Llama 3.2 1B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85c01e52-989a-4439-82dd-67a3d3ab4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was an evil wizard called Lord Gogul. He lived in the middle of nowhere on earth, and his only goal in life seemed to be world domination.\n",
      "One day he heard about this strange man who had recently come back from outer space with a huge collection of new ideas which would turn the entire planet into one big mess! So off Lord Gogul went seeking this mysterious stranger's advice...\n",
      "He found him sitting outside a small hut surrounded by a crowd of gullible followers waiting for his answers!\n",
      "\"Dear friend,\" said the Wizard as they exchanged pleasantries, \"you are here today because I hear you have many great plans for our good country - how do we know that it isn't just your ego talking?\"\n",
      "The young inventor looked down at himself quizzically before replying: \"No sir! It is my whole body covered...\"\n",
      "So the Great Leader sat up close enough so their faces were nearly touching...and then whispered something very carefully into\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Our Story Prompt\n",
    "story_prompt = \"Once upon a time\"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-1B\n",
    "outputs = model.generate(prompt_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         max_length=200,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         pad_token_id=tokenizer.eos_token_id,\n",
    "                         temperature=0.93,\n",
    "                         top_k=30,\n",
    "                         top_p=0.90,\n",
    "                         repetition_penalty=1.2\n",
    "                        )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0]\n",
    "\n",
    "generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fd9a8-470d-4760-9fcb-675a985c6098",
   "metadata": {},
   "source": [
    "**Measure of Perplexity**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39b29aaa-69c0-4ef3-ab17-6f0e91e6f512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Llama 3.2 1B Model: 501791.19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs[\"input_ids\"][:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Llama 3.2 1B Model: {round(perplexity.item(),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c0115-5a20-460c-9aa0-209da6b7c6a6",
   "metadata": {},
   "source": [
    "**Measure Token Type Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60996167-f478-44b6-98ce-761815e4af56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Llama 3.2 1B Model: 1.0\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Once upon a time\"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Llama 3.2 1B Model:\", round(ttr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c01c09-fb8e-4274-bba2-11192240b20a",
   "metadata": {},
   "source": [
    "### Llama 3.2 - 3B:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff71f1-bd69-456d-801b-87d6aabd0fa4",
   "metadata": {},
   "source": [
    "**Download Llama 3.2 - 3B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27ce93bf-9ff2-4170-b8db-0fd3006476cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:26<00:00, 13.01s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_3B = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_3B)\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_32_3B, torch_dtype=torch.float32)\n",
    "\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb750382-256e-469b-854c-9c43f0fbcd26",
   "metadata": {},
   "source": [
    "**Generate A Story with Llama 3.2 3B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85770997-9892-4d17-97e2-9c04c7f5fbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time 5th century BCE, the Indian sage Gautama Buddha was asked to explain how to overcome sorrow. The Lord said that it is through detachment from worldly pleasures and attachments that one can achieve happiness in this life.\n",
      "The wise man who does not desire anything other than truth knows neither attachment nor disattachment; he has no fear of loss or gain. In him there are no worries about what may happen tomorrow (Sri Lanka Times).\n",
      "Detachment: What It Is\n",
      "In Buddhist philosophy, detaching oneself means living without any desires for material possessions such as money, fame, power etc., because these things do not bring true happiness but only temporary pleasure which ultimately leads us towards suffering when they disappear!\n",
      "It’s easy enough – just say “no”! But Buddhism teaches something much deeper than denying ourselves certain kinds of wants like sex or drugs…instead we must learn how to let go of everything except our own inner peace within each moment so that even when external circumstances change around\n"
     ]
    }
   ],
   "source": [
    "# Your special prompt\n",
    "story_prompt = \"Once upon a time \"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-3B\n",
    "outputs = model.generate(prompt_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=200,\n",
    "                        do_sample=True,\n",
    "                        num_return_sequences=1,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        temperature=0.83,\n",
    "                        top_k=30,\n",
    "                        top_p=0.90,\n",
    "                        repetition_penalty=1.2\n",
    "                    )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0]\n",
    "\n",
    "generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf467d5a-2c07-46ae-b660-3d579d7eb214",
   "metadata": {},
   "source": [
    "**Measure Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f8c739b-4676-4d26-ae16-cfbe8e5eb1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Llama 3.2 3B Model: 232412.89\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs[\"input_ids\"][:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Llama 3.2 3B Model: {round(perplexity.item(),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c18db-a6d4-465a-867f-e62ae3e1585b",
   "metadata": {},
   "source": [
    "**Measure Type-Token Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e97e1d68-aa02-457b-8cef-7e738360f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Llama 3.2 3B Model: 1.0\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Once upon a time\"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Llama 3.2 3B Model:\", round(ttr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70271cf-668e-4065-a838-783846cac664",
   "metadata": {},
   "source": [
    "### Phi 3.5-Mini-Instruct:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d765aff-1082-44fc-ab36-ba5a2e8e2404",
   "metadata": {},
   "source": [
    "**Download Phi 3.5 - Mini - Instruct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6708f995-eaae-4700-9275-eb1aec36d677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.62s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a 200 word story that begins with, 'Once upon a time' and ends with, 'and they lived happily ever after.' The story must include a dragon, a lost crown, and a mysterious forest. Ensure that the narrative has a clear beginning, middle, and end, and that the dragon plays a crucial role in the resolution of the plot.\n",
      "\n",
      "\n",
      "### Answer: Once upon a time, in a kingdom shadowed by a mysterious forest, a crown of unparalleled beauty was lost. The king, distraught, declared a reward for its return. Among the villagers, a young shepherd named Eli ventured into the forest, guided by whispers of the dragon, Drako, who was rumored to guard the crown.\n",
      "\n",
      "\n",
      "Deep within the woods, Eli encountered Drako, whose scales shimmered like emeralds. The dragon, initially wary, sensed Eli's pure heart and revealed the crown's hiding place, entangled in a thicket. Eli retrieved it with care, and in gratitude, promised to protect the forest.\n",
      "\n",
      "\n",
      "The king, overjoyed, bestowed the crown upon Eli, who became a symbol of unity between the kingdom and the forest. The dragon, now an ally, watched over the land. And they lived happily ever after.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "phi_35_mini_inst = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(phi_35_mini_inst)\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(phi_35_mini_inst)\n",
    "\n",
    "\n",
    "prompt_text = \"Generate a 200 word story that begins with, 'Once upon a time'\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = phi_tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = phi_model.generate(inputs.input_ids, max_length=200, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_story = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90437347-5282-4ded-b722-b19cd0ba677d",
   "metadata": {},
   "source": [
    "**Measure of Perplexity**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd07637-1ee4-4e80-8a30-40fbcdf062d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Phi-3 Model: 6.56\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = phi_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs[\"input_ids\"][:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Phi-3 Model: {round(perplexity.item(),2)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16befdc5-5305-494b-b6ea-c88fd27e0b48",
   "metadata": {},
   "source": [
    "**Measure Type Token Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e0c5d2-b221-42c4-8e60-df0e354edc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Phi-3 Model: 0.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_text = \"Generate a 200 word story that begins with, 'Once upon a time'\"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Phi-3 Model:\", round(ttr,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
