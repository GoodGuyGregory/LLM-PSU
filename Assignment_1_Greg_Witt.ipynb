{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051412af-c8e9-46be-90ea-9966e47db70d",
   "metadata": {},
   "source": [
    "# Assignment 1: Large Language Models for Text Generation\n",
    "### CS 410/510 Large Language Models Fall 2024\n",
    "#### Greg Witt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c8544-965b-41c7-9c42-1232265ea4bf",
   "metadata": {},
   "source": [
    "### Q1. Describe three differences between Llama 3.2 models and Phi-3.5 model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c29179-cec8-417b-9f83-c2e895bdabc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410cb3c6-93db-4d8a-a680-69118ed2bee5",
   "metadata": {},
   "source": [
    "### Q2. Generate a story of 200 words that starts with the words *“Once upon a time”* using each of these models.  \n",
    "**You should have 3 outputs in total.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e5fbf-5e05-412a-a0df-ae31a5254d04",
   "metadata": {},
   "source": [
    "Below are three instances of the requested models. Each was executed **three times** the last run is featured below the model's generation cell. the **additional stories** are featured *below* the final **Llama** model and the **Phi** model. the link will take you to a git repo that has the images stored. \n",
    "\n",
    "an **analysis** will below each model and an in depth explaination will be featured there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8b1db",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aabfb-4720-4baf-b2ad-cb32f59b100e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# pip install transformers\n",
    "# pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f3c4f-00d1-4b6e-92a5-6006da1b7612",
   "metadata": {},
   "source": [
    "### Llama 3.2 - 1B:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a9a35-4f05-4f07-a00f-e028db3eb618",
   "metadata": {},
   "source": [
    "**Download Llama-3.2 1B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b235451-866b-42c0-99d0-64446bf8d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_1B = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "llama_32_1b_tokenizer = AutoTokenizer.from_pretrained(llama_32_1B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "llama_32_1b_tokenizer.pad_token_id = llama_32_1b_tokenizer.eos_token_id\n",
    "\n",
    "llama_32_1b_model = AutoModelForCausalLM.from_pretrained(llama_32_1B, torch_dtype=torch.float16)\n",
    "\n",
    "llama_32_1b_model = llama_32_1b_model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fc306-1eae-4907-8370-4c61e6064449",
   "metadata": {},
   "source": [
    "**Generate A Story with Llama 3.2 1B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c01e52-989a-4439-82dd-67a3d3ab4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was the King of all Men and his beautiful Wife. He gave her an apple from which he believed she would never eat anything again.\n",
      "She began to cry when it first hit her mouth but then stopped as soon as her lips touched that magical fruit – and so did everything else in sight! The King had lost hope for another woman’s love forever until one day…\n",
      "…He woke up with 42 women!\n",
      "That morning, while everyone went about their usual business; breakfasts were made,\n",
      "lunches eaten\n",
      "dinnners served &\n",
      "all this took place without even missing out on brushing your teeth or taking off those damn makeup brushes you use every night before bed just because they smell nice?\n",
      "There was no need at all since none of them could be found anywhere near us anymore either! All over town these days? No problemo!\n",
      "All we needed here today though came right down front & personal too — only thing anyone knew who lived alone now more often than\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Our Story Prompt\n",
    "story_prompt = \"Once upon a time\"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = llama_32_1b_tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(llama_32_1b_tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-1B\n",
    "outputs = llama_32_1b_model.generate(prompt_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         max_length=200,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         pad_token_id=llama_32_1b_tokenizer.eos_token_id,\n",
    "                         temperature=0.93,\n",
    "                         top_k=30,\n",
    "                         top_p=0.90,\n",
    "                         repetition_penalty=1.2\n",
    "                        )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0]\n",
    "\n",
    "generated_story = llama_32_1b_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fd9a8-470d-4760-9fcb-675a985c6098",
   "metadata": {},
   "source": [
    "**Measure of Perplexity**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b29aaa-69c0-4ef3-ab17-6f0e91e6f512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Llama 3.2 1B Model: 14.88\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = llama_32_1b_model(prompt_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = prompt_ids[:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=llama_32_1b_tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Llama 3.2 1B Model: {round(perplexity.item(),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c0115-5a20-460c-9aa0-209da6b7c6a6",
   "metadata": {},
   "source": [
    "**Measure Token Type Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60996167-f478-44b6-98ce-761815e4af56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Llama 3.2 1B Model: 1.0\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Once upon a time \"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Llama 3.2 1B Model:\", round(ttr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a7633-e583-49bd-9fb8-246cb3b0bc35",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c01c09-fb8e-4274-bba2-11192240b20a",
   "metadata": {},
   "source": [
    "### Llama 3.2 - 3B:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff71f1-bd69-456d-801b-87d6aabd0fa4",
   "metadata": {},
   "source": [
    "**Download Llama 3.2 - 3B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ce93bf-9ff2-4170-b8db-0fd3006476cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9821db514d2043b7b15d50a595569548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "llama_32_3B = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# creates a Tokenizer specifically for the llama_model requested\n",
    "llama_32_3b_tokenizer = AutoTokenizer.from_pretrained(llama_32_3B)\n",
    "\n",
    "# Set the padding token ID to be the same as the EOS token ID\n",
    "llama_32_3b_tokenizer.pad_token_id = llama_32_3b_tokenizer.eos_token_id\n",
    "\n",
    "llama_32_3b_model = AutoModelForCausalLM.from_pretrained(llama_32_3B, torch_dtype=torch.float32)\n",
    "\n",
    "llama_32_3b_model = llama_32_3b_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb750382-256e-469b-854c-9c43f0fbcd26",
   "metadata": {},
   "source": [
    "**Generate A Story with Llama 3.2 3B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85770997-9892-4d17-97e2-9c04c7f5fbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time 2.0: The second coming of the world's most famous animated film\n",
      "By Chris HallamPosted on July 1, 2015 August 3, 2020 Posted in Film reviewsTagged Disney, Donald Duck, Fantasia, Mickey Mouse, Pinocchio, Snow White and the Seven Dwarfs No Comments on Once upon a time 2.0: The second coming of the world’s most famous animated film\n",
      "Snow White and the Seven Dwarfs was released to critical acclaim in America by Walt Disney Pictures (or as they were then known – Laugh-O-Gram Studio) way back in December 1937. It remains arguably one of cinema’s greatest achievements. Its success paved the road for all future Disney animation productions.\n",
      "The original soundtrack had been so popular that it spawned an album which featured three tracks from the movie. These included “Someday My Prince Will Come”, “Whistle While You Work” and “Heigh\n"
     ]
    }
   ],
   "source": [
    "# Your special prompt\n",
    "story_prompt = \"Once upon a time \"\n",
    "    \n",
    "# Encode the prompt into token IDs\n",
    "prompt_ids = llama_32_3b_tokenizer.encode(story_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = prompt_ids.ne(llama_32_3b_tokenizer.pad_token_id)\n",
    "\n",
    "# Generate a response from llama_3.2-3B\n",
    "outputs = llama_32_3b_model.generate(prompt_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=200,\n",
    "                        do_sample=True,\n",
    "                        num_return_sequences=1,\n",
    "                        pad_token_id=llama_32_3b_tokenizer.eos_token_id,\n",
    "                        temperature=0.83,\n",
    "                        top_k=30,\n",
    "                        top_p=0.90,\n",
    "                        repetition_penalty=1.2\n",
    "                    )\n",
    "\n",
    "# Decode the generated response\n",
    "generated_tokens = outputs[0]\n",
    "\n",
    "generated_story = llama_32_3b_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf467d5a-2c07-46ae-b660-3d579d7eb214",
   "metadata": {},
   "source": [
    "**Measure Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8c739b-4676-4d26-ae16-cfbe8e5eb1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Llama 3.2 3B Model: 35.32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = llama_32_3b_model(prompt_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = prompt_ids[:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=llama_32_3b_tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Llama 3.2 3B Model: {round(perplexity.item(),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c18db-a6d4-465a-867f-e62ae3e1585b",
   "metadata": {},
   "source": [
    "**Measure Type-Token Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e97e1d68-aa02-457b-8cef-7e738360f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Llama 3.2 3B Model: 1.0\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Once upon a time\"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Llama 3.2 3B Model:\", round(ttr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1056bd-4325-402c-9a7a-c528dec82364",
   "metadata": {},
   "source": [
    " **[Additional Stories](https://github.com/GoodGuyGregory/Llama-3.2-vs-Phi-3/tree/token_check/img/llama3.2)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbd093-4302-4eb7-abbd-40b446a05dc2",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70271cf-668e-4065-a838-783846cac664",
   "metadata": {},
   "source": [
    "### Phi 3.5-Mini-Instruct:\n",
    "\n",
    "[Hugging Face Model Card](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d765aff-1082-44fc-ab36-ba5a2e8e2404",
   "metadata": {},
   "source": [
    "**Download Phi 3.5 - Mini - Instruct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6708f995-eaae-4700-9275-eb1aec36d677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████| 2/2 [00:15<00:00,  7.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "phi_35_mini_inst = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(phi_35_mini_inst)\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(phi_35_mini_inst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a359bf-0d4e-41cd-8d7f-4fb7400a1e1b",
   "metadata": {},
   "source": [
    "**Generate a Story with Phi 3.5 - Mini Instruct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b4c3dd-93ea-4371-8143-a0c39ec85f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a 200 word story that begins with, 'Once upon a time' incorporate a nature theme and make it scary, and mysterious in mood, without using words related to 'fear', 'dark', 'mysterious', 'scary', 'night', or 'monster'.\n",
      "\n",
      "In a dense, shadow-draped forest, where sunlight seldom danced through the ancient canopy, there existed a silence so profound that even the whispers of the wind seemed subdued. Once upon a time, under this ethereal stillness, a tale unfurled, woven from threads of enigma and the haunting beauty of nature itself.\n",
      "\n",
      "An old oak, gnarled with secrets and wisdom from ages past, stood solitary amidst its brethren. Its bark bore the intricate carvings of forgotten lore, and its hollows whisper\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Generate a 200 word story that begins with, 'Once upon a time' incorporate a nature theme and make it scary, and mysterious\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = phi_tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = phi_model.generate(inputs.input_ids, \n",
    "                             max_length=200,\n",
    "                             temperature=0.85,\n",
    "                             do_sample=True\n",
    "                             )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_story = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90437347-5282-4ded-b722-b19cd0ba677d",
   "metadata": {},
   "source": [
    "**Measure of Perplexity**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd07637-1ee4-4e80-8a30-40fbcdf062d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Phi-3 Model: 18.66\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the Logits from the Model based on the Inputs\n",
    "with torch.no_grad():\n",
    "    outputs = phi_model(inputs.input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift the input_ids to the right to determine the next token for the model to predict\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs.input_ids[:, 1:].contiguous()\n",
    "\n",
    "# calculate the log likelihood based on Cross EntropyLoss\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "# determine the loss value to exponentiate\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "# exponentiate the loss\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "# return the results\n",
    "print(f\"Perplexity for Phi-3 Model: {round(perplexity.item(),2)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16befdc5-5305-494b-b6ea-c88fd27e0b48",
   "metadata": {},
   "source": [
    "**Measure Type Token Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e0c5d2-b221-42c4-8e60-df0e354edc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token Ratio (TTR) for Phi-3 Model: 0.86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_text = \"Generate a 200 word story that begins with, 'Once upon a time' incorporate a nature theme and make it scary, and mysterious\"\n",
    "\n",
    "tokens = prompt_text.split()\n",
    "\n",
    "types = set(tokens)\n",
    "ttr = len(types) / len(tokens)\n",
    "\n",
    "print(\"Type-Token Ratio (TTR) for Phi-3 Model:\", round(ttr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ff3a3-25a9-4a82-bccb-296f2b5f23c1",
   "metadata": {},
   "source": [
    "**[Additional Stories](https://github.com/GoodGuyGregory/Llama-3.2-vs-Phi-3/tree/token_check/img/phi3.5)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe7f83-3a4f-4eb3-b38e-6871a102903c",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc207a",
   "metadata": {},
   "source": [
    "## Q3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3d8a8-b4f0-4b6d-8d34-86be2e53626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
